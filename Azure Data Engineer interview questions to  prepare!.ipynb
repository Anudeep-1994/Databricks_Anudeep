{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "894bbe30-c4d4-4983-9e34-181b66ff0408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1.Parameters and Variables in ADF:\n",
    "----------------------------------\n",
    "\n",
    "‚Ä¢ Parameters: Used to pass values at runtime to pipelines. They are \n",
    "defined at the pipeline level and cannot be changed during execution.\n",
    "\n",
    "‚Ä¢ Variables: Used to store values within the pipeline and can change \n",
    "during execution. Variables are updated using Set Variable or Append\n",
    "Variable activities\n",
    "\n",
    "\n",
    "\n",
    "2.Time Travel in Your Project:\n",
    "------------------------------\n",
    "\n",
    "‚Ä¢ Time travel is a Delta Lake feature that allows querying historical data \n",
    "(snapshots).\n",
    "\n",
    "‚Ä¢ Example: SELECT * FROM table_name VERSION AS OF 5 or TIMESTAMP AS OF\n",
    "'2023-01-15' .\n",
    "\n",
    "‚Ä¢ Use Case: Debugging, auditing, or recreating datasets for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af199407-f767-4fc9-942e-5616da271f09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Page 1:\n",
    "\n",
    "Key Answers: \n",
    "------------\n",
    "\n",
    "1Ô∏è‚É£ Parameters and Variables in ADF: \n",
    "\n",
    "‚Ä¢ Parameters: Used to pass values at runtime to pipelines. They are \n",
    "defined at the pipeline level and cannot be changed during execution. \n",
    "\n",
    "‚Ä¢ Variables: Used to store values within the pipeline and can change \n",
    "during execution. Variables are updated using Set Variable or Append \n",
    "Variable activities. \n",
    "\n",
    "\n",
    "2Ô∏è‚É£ Time Travel in Your Project: \n",
    "\n",
    "‚Ä¢ Time travel is a Delta Lake feature that allows querying historical data \n",
    "(snapshots). \n",
    "\n",
    "‚Ä¢ Example: SELECT * FROM table_name VERSION AS OF 5  or TIMESTAMP AS OF  \n",
    "'2023-01-15'.\n",
    "\n",
    "‚Ä¢ Use Case: Debugging, auditing, or recreating datasets for ML models. \n",
    "\n",
    "\n",
    "3Ô∏è‚É£ Resume Pipeline from Failed Activity: \n",
    "\n",
    "‚Ä¢ Enable checkpointing or activity retry in ADF. Use a failure path with \n",
    "logic to resume execution by using the Get Metadata activity to \n",
    "evaluate where the pipeline failed. \n",
    "\n",
    "4Ô∏è‚É£ Pipelines You've Worked With: \n",
    "    \n",
    "‚Ä¢ Example: ETL pipelines to ingest and transform raw data from Azure \n",
    "Data Lake using Data Flows and Spark jobs. \n",
    "‚Ä¢ Mention specifics like copy data activities, data validation, \n",
    "and orchestration of transformations. \n",
    "\n",
    "5Ô∏è‚É£ Partition vs. Bucketing: \n",
    "    \n",
    "‚Ä¢ Partitioning: Divides the data into directories based on keys (e.g., year, \n",
    "month). \n",
    "\n",
    "‚Ä¢ Bucketing: Hashes data into fixed-sized buckets, optimizing joins and \n",
    "aggregations. \n",
    "\n",
    "\n",
    "Page 2:\n",
    "--------\n",
    "    \n",
    "    \n",
    "6Ô∏è‚É£ Medallion Architecture: \n",
    "    \n",
    "‚Ä¢ A data architecture that separates data into three layers: \n",
    "o Bronze: Raw ingested data. \n",
    "o Silver: Cleaned and transformed data. \n",
    "o Gold: Business-level aggregates and insights. \n",
    "\n",
    "\n",
    "7Ô∏è‚É£ Azure Key Vault: \n",
    "    \n",
    "‚Ä¢ Securely stores secrets, keys, and certificates. \n",
    "‚Ä¢ Use Managed Identity in ADF to access Key Vault without hardcoding \n",
    "credentials. \n",
    "\n",
    "8Ô∏è‚É£ Unity Catalog vs. Hive Metastore: \n",
    "    \n",
    "‚Ä¢ Unity Catalog: Centralized data governance and access control for all \n",
    "your Databricks workspaces. \n",
    "\n",
    "‚Ä¢ Hive Metastore: Manages metadata for Hive and Spark tables, but lacks \n",
    "robust access control. \n",
    "\n",
    "9Ô∏è‚É£ Joins in PySpark: \n",
    "‚Ä¢ Inner Join: Matches rows from both datasets based on a condition. \n",
    "‚Ä¢ Left/Right Join: Keeps all rows from the left/right and matches with \n",
    "the right/left dataset. \n",
    "‚Ä¢ Full Outer Join: Includes all rows from both datasets. \n",
    "‚Ä¢ Cross Join: Cartesian product of both datasets. \n",
    "\n",
    "\n",
    " \n",
    "üîü How to Implement Parallel Processing in ADF? \n",
    "\n",
    "‚Ä¢ Use For Each Activity with the Batch Count property set for \n",
    "parallelism. \n",
    "\n",
    "\n",
    "Page 3:\n",
    "    \n",
    "‚Ä¢ Enable concurrent execution in pipeline settings. \n",
    "‚Ä¢ Use partitioned datasets for parallel reads/writes to optimize \n",
    "execution. \n",
    " \n",
    "1Ô∏è‚É£1Ô∏è‚É£ Difference Between Narrow and Wide Transformations: \n",
    "    \n",
    "‚Ä¢ Narrow: Data is processed within the same partition (e.g., map, filter). \n",
    "Minimal shuffling. \n",
    "\n",
    "‚Ä¢ Wide: Data is shuffled across partitions (e.g., groupBy, join). Higher \n",
    "computational cost. \n",
    " \n",
    "1Ô∏è‚É£2Ô∏è‚É£ What is SCD? Explain SCD1, SCD2, SCD3: \n",
    "    \n",
    "‚Ä¢ SCD (Slowly Changing Dimensions) handles historical changes in \n",
    "dimension data. \n",
    "‚Ä¢ SCD1: Overwrites old data with new data. \n",
    "‚Ä¢ SCD2: Maintains history by adding new rows for changes (e.g., adding \n",
    "an Effective_Date ). \n",
    "‚Ä¢ SCD3: Adds new columns to store historical data for specific attributes. \n",
    " \n",
    "1Ô∏è‚É£3Ô∏è‚É£ Cluster Options in Databricks: \n",
    "    \n",
    "‚Ä¢ Standard Cluster: For general-purpose workloads. \n",
    "‚Ä¢ High-Concurrency Cluster: Optimized for multiple concurrent users. \n",
    "‚Ä¢ Single Node Cluster: For lightweight testing and debugging. \n",
    "‚Ä¢ Jobs Cluster: Automatically created for specific jobs and deleted \n",
    "afterward. \n",
    " \n",
    "1Ô∏è‚É£4Ô∏è‚É£ Difference Between Managed and External Tables: \n",
    "    \n",
    "‚Ä¢ Managed Tables: Databricks manages the data and metadata (stored in \n",
    "default storage). \n",
    "\n",
    "‚Ä¢ External Tables: Data is stored outside Databricks, and only metadata \n",
    "is managed in the metastore. \n",
    " \n",
    "\n",
    "\n",
    "Page 4:\n",
    "    \n",
    "1Ô∏è‚É£5Ô∏è‚É£ What is a Surrogate Key?\n",
    "\n",
    "‚Ä¢ A unique identifier for a record, not derived from application data. \n",
    "‚Ä¢ Example: Auto-increment ID in databases. \n",
    " \n",
    "1Ô∏è‚É£6Ô∏è‚É£ Spark Optimization Techniques: \n",
    "  \n",
    "‚Ä¢ Cache/persist frequently used data. \n",
    "‚Ä¢ Use broadcast joins for smaller datasets. \n",
    "‚Ä¢ Partition data effectively. \n",
    "‚Ä¢ Enable predicate pushdown for filters. \n",
    "‚Ä¢ Avoid wide transformations where possible. \n",
    " \n",
    "1Ô∏è‚É£7Ô∏è‚É£ Why is Databricks Better Than Dataflow? \n",
    "\n",
    "‚Ä¢ Flexibility: Databricks supports more complex workloads (e.g., ML, \n",
    "streaming). \n",
    "‚Ä¢ Notebook Interface: Collaborative development environment. \n",
    "‚Ä¢ Performance: Databricks uses Apache Spark with optimizations like \n",
    "Delta Lake. \n",
    "‚Ä¢ Dataflow is simpler for straightforward ETL use cases. \n",
    " \n",
    "1Ô∏è‚É£8Ô∏è‚É£ Difference Between Data Lake and Delta Lake: \n",
    "    \n",
    "‚Ä¢ Data Lake: Stores raw, unstructured data. No ACID compliance. \n",
    "\n",
    "‚Ä¢ Delta Lake: Built on top of a data lake with ACID transactions, time \n",
    "travel, and schema enforcement. \n",
    " \n",
    "1Ô∏è‚É£9Ô∏è‚É£ Explain Spark Architecture: \n",
    "    \n",
    "‚Ä¢ Driver: Coordinates execution, maintains DAG, and schedules tasks. \n",
    "‚Ä¢ Executors: Run tasks assigned by the driver. Each executor has its \n",
    "memory and cache. \n",
    "‚Ä¢ Cluster Manager: (e.g., YARN, Kubernetes) Allocates resources to the \n",
    "driver and executors. \n",
    "\n",
    "\n",
    "Page 5:\n",
    " \n",
    "2Ô∏è‚É£0Ô∏è‚É£ Difference Between groupByKey and reduceByKey: \n",
    "    \n",
    "‚Ä¢ groupByKey: Groups all key-value pairs by key and shuffles all data. \n",
    "More memory-intensive. \n",
    "‚Ä¢ reduceByKey: Combines values at the mapper side before shuffling, \n",
    "reducing network traffic. Preferred for better performance. \n",
    " \n",
    "2Ô∏è‚É£1Ô∏è‚É£ Why is MapReduce Not Widely Used Now? Similarities Between \n",
    "Spark and MapReduce? \n",
    "\n",
    "‚Ä¢ Why not MapReduce: \n",
    "o High latency due to disk I/O for intermediate results. \n",
    "o Complex to code compared to Spark. \n",
    "\n",
    "‚Ä¢ Similarities: \n",
    "o Both process large-scale data using distributed computing. \n",
    "o Use key-value pairs for transformations. \n",
    "\n",
    "‚Ä¢ Spark Advantages: \n",
    "o In-memory computation, faster execution, rich APIs (Python, \n",
    "Scala). \n",
    " \n",
    "2Ô∏è‚É£2Ô∏è‚É£ What is Delta Lake? Key Features and Creating Delta Tables: \n",
    "    \n",
    "‚Ä¢ Delta Lake: A storage layer on top of Data Lake offering ACID \n",
    "compliance and reliability. \n",
    "\n",
    "‚Ä¢ Key Features: \n",
    "o ACID transactions. \n",
    "o Schema enforcement and evolution. \n",
    "o Time travel and versioning. \n",
    "\n",
    "\n",
    "‚Ä¢ Creating Delta Tables: \n",
    "\n",
    "\n",
    "Page 6:\n",
    "CREATE TABLE delta_table USING DELTA LOCATION 'path_to_delta'; \n",
    " \n",
    "2Ô∏è‚É£3Ô∏è‚É£ Difference Between Serverless Pool and Dedicated SQL Pool: \n",
    "    \n",
    "‚Ä¢ Serverless Pool: \n",
    "o Pay-per-query model. \n",
    "o Used for ad-hoc queries on data lakes. \n",
    "\n",
    "‚Ä¢ Dedicated SQL Pool: \n",
    "o Pre-provisioned resources with fixed cost. \n",
    "o Designed for high-performance data warehousing. \n",
    " \n",
    "2Ô∏è‚É£4Ô∏è‚É£ Prerequisites Before Migration: \n",
    "    \n",
    "‚Ä¢ Assess source and target environments. \n",
    "‚Ä¢ Ensure schema compatibility. \n",
    "‚Ä¢ Perform data profiling and cleansing. \n",
    "‚Ä¢ Set up network, storage, and permissions. \n",
    "‚Ä¢ Validate data transformation logic. \n",
    " \n",
    "\n",
    "2Ô∏è‚É£5Ô∏è‚É£ What is a Mount Point in Databricks? \n",
    "\n",
    "‚Ä¢ A mount point is a shortcut to a storage account, enabling easier \n",
    "access. \n",
    "‚Ä¢ Example: Mounting an Azure Data Lake Gen2 folder using \n",
    "a dbutils.fs.mount  command. \n",
    " \n",
    "2Ô∏è‚É£6Ô∏è‚É£ How to Optimize Databricks Performance: \n",
    "    \n",
    "‚Ä¢ Enable Delta Lake optimizations like Z-ordering and OPTIMIZE. \n",
    "‚Ä¢ Use Auto-scaling for clusters. \n",
    "‚Ä¢ Use broadcast joins for smaller datasets. \n",
    "‚Ä¢ Optimize shuffling with correct partitioning. \n",
    "‚Ä¢ Persist reusable datasets in memory with cache() . \n",
    " \n",
    "\n",
    "\n",
    "Page 7:\n",
    "    \n",
    "2Ô∏è‚É£7Ô∏è‚É£ Difference Between map and flatMap: \n",
    "    \n",
    "‚Ä¢ map: Transforms each element into another element, 1-to-1 mapping. \n",
    "‚Ä¢ flatMap: Can produce 0 or more elements per input, 1-to-n mapping. \n",
    " \n",
    "2Ô∏è‚É£8Ô∏è‚É£ How to Fetch Details from Key Vault: \n",
    "    \n",
    "‚Ä¢ Use Azure Key Vault Linked Service in ADF or Databricks. \n",
    "‚Ä¢ In Databricks: \n",
    "secret_value = dbutils.secrets.get(scope=\"key_vault_scope\", \n",
    "key=\"secret_name\") \n",
    " \n",
    "2Ô∏è‚É£9Ô∏è‚É£ Applying Indexing on a Databricks Table:\n",
    "     \n",
    "‚Ä¢ Use Delta Lake Z-order indexing: \n",
    "OPTIMIZE delta_table_name ZORDER BY (column_name); \n",
    "‚Ä¢ Helps improve query performance for large datasets. \n",
    " \n",
    "\n",
    "3Ô∏è‚É£0Ô∏è‚É£ Transferring Data to Azure Synapse: \n",
    "    \n",
    "‚Ä¢ Use Azure Data Factory for ETL pipelines. \n",
    "‚Ä¢ COPY INTO command in Synapse for fast ingestion from Data Lake. \n",
    "‚Ä¢ Databricks-to-Synapse via JDBC connector or PolyBase. \n",
    " \n",
    "\n",
    " \n",
    "3Ô∏è‚É£1Ô∏è‚É£ What is Incremental Loading? How to Implement It? \n",
    "\n",
    "‚Ä¢ Definition: Loading only new or updated data to a target without \n",
    "reloading the entire dataset. \n",
    "\n",
    "\n",
    "Page 8:\n",
    "    \n",
    "‚Ä¢ Implementation: \n",
    "  --------------\n",
    "\n",
    "o Watermarking: Use timestamps or surrogate keys to identify \n",
    "changes. \n",
    "o ADF: Use Lookup + Filter activities. \n",
    "o Delta Lake: Merge using UPSERT logic: \n",
    "o MERGE INTO target_table AS target \n",
    "o USING source_table AS source \n",
    "o ON target.id = source.id \n",
    "o WHEN MATCHED THEN UPDATE SET target.col = source.col \n",
    "WHEN NOT MATCHED THEN INSERT (columns) VALUES (values); \n",
    " \n",
    "3Ô∏è‚É£2Ô∏è‚É£ How Does Z-Ordering Work? \n",
    "\n",
    "‚Ä¢ Z-Ordering: A data layout optimization in Delta Lake that reduces I/O \n",
    "by co-locating similar data on disk. \n",
    "\n",
    "‚Ä¢ How: \n",
    "o Applies a multi-dimensional sort algorithm. \n",
    "o Improves query performance on frequently filtered columns. \n",
    "OPTIMIZE table_name ZORDER BY (column1, column2); \n",
    " \n",
    "3Ô∏è‚É£3Ô∏è‚É£ What is Dimension Modeling? Dimension and Fact Tables? \n",
    "\n",
    "‚Ä¢ Dimension Modeling: A design technique for data warehouses to \n",
    "optimize query performance using star or snowflake schemas. \n",
    "\n",
    "‚Ä¢ Fact Tables: Store numeric measures (e.g., sales amount). \n",
    "‚Ä¢ Dimension Tables: Describe the context of facts (e.g., customer, \n",
    "product). \n",
    " \n",
    "3Ô∏è‚É£4Ô∏è‚É£ Difference Between a Data Lake and a Data Warehouse: \n",
    "  \n",
    "‚Ä¢ Data Lake: \n",
    "\n",
    "Page 9:\n",
    "o Stores raw, unstructured data. \n",
    "o Scalable, cost-effective. \n",
    "o Example: Azure Data Lake. \n",
    "\n",
    "‚Ä¢ Data Warehouse: \n",
    "o Stores structured, processed data for analytics. \n",
    "o Schema-on-write. \n",
    "o Example: Azure Synapse. \n",
    " \n",
    "3Ô∏è‚É£5Ô∏è‚É£ Using Logic Apps in Your Project: \n",
    "  \n",
    "‚Ä¢ Automates workflows between services like ADF, Synapse, and \n",
    "notifications. \n",
    "‚Ä¢ Example Use Case: \n",
    "o Trigger data pipelines based on events (e.g., file upload). \n",
    "o Send failure alerts via email or Teams. \n",
    " \n",
    "3Ô∏è‚É£6Ô∏è‚É£ What is Data Skewness? \n",
    "‚Ä¢ Definition: Uneven distribution of data across partitions, leading to \n",
    "performance bottlenecks. \n",
    "\n",
    "‚Ä¢ Mitigation: \n",
    "o Use salting techniques (adding random keys). \n",
    "o Optimize partitioning with balanced keys. \n",
    " \n",
    "3Ô∏è‚É£7Ô∏è‚É£ What is Fault Tolerance and Its Use in Real-Time Applications? \n",
    "\n",
    "‚Ä¢ Definition: The ability of a system to recover from failures. \n",
    "‚Ä¢ Real-Time Use: \n",
    "o Spark achieves fault tolerance by storing lineage and \n",
    "recomputing lost partitions. \n",
    "o In ADF, retry policies handle transient failures. \n",
    " \n",
    "3Ô∏è‚É£8Ô∏è‚É£ Converting RDD to DataFrame & Vice Versa: \n",
    "\n",
    "\n",
    "Page 10:\n",
    "‚Ä¢ RDD to DataFrame: \n",
    "‚Ä¢ from pyspark.sql import SparkSession \n",
    "df = rdd.toDF(schema=[\"col1\", \"col2\"]) \n",
    "\n",
    "‚Ä¢ DataFrame to RDD: \n",
    "rdd = df.rdd \n",
    " \n",
    "3Ô∏è‚É£9Ô∏è‚É£ Encryption Techniques: \n",
    "  \n",
    "‚Ä¢ At Rest: Encrypt data in storage using Azure Storage Service \n",
    "Encryption (SSE). \n",
    "\n",
    "‚Ä¢ In Transit: Use TLS/SSL for secure data transfer. \n",
    "‚Ä¢ Column-Level Encryption: Secure sensitive data fields (e.g., PII). \n",
    " \n",
    "4Ô∏è‚É£0Ô∏è‚É£ How Does Auto Loader Work? \n",
    "‚Ä¢ A feature in Databricks for incremental file processing from cloud \n",
    "storage. \n",
    "\n",
    "‚Ä¢ Working: \n",
    "o Tracks metadata using checkpointing. \n",
    "o Processes new files automatically. \n",
    "\n",
    "‚Ä¢ Example: \n",
    "‚Ä¢ df = spark.readStream.format(\"cloudFiles\") \\ \n",
    "    .option(\"cloudFiles.format\", \"json\") \\ \n",
    "    .load(\"path\") \n",
    " \n",
    "4Ô∏è‚É£1Ô∏è‚É£ Explain Lazy Evaluation in PySpark: \n",
    "\n",
    "\n",
    "Page 11:\n",
    "  \n",
    "‚Ä¢ Definition: Transformations are not executed immediately but only \n",
    "when an action (e.g., count , collect ) is triggered. \n",
    "\n",
    "‚Ä¢ Benefits: \n",
    "o Optimizes execution by combining transformations into a single \n",
    "stage. \n",
    "o Reduces unnecessary computations. \n",
    "  \n",
    "4Ô∏è‚É£2Ô∏è‚É£ What is DAG in Spark? \n",
    "\n",
    "‚Ä¢ DAG (Directed Acyclic Graph): \n",
    "  \n",
    "o A sequence of computations where each node represents a \n",
    "transformation and edges represent dependencies. \n",
    "o Spark breaks the execution into stages using DAG, ensuring fault \n",
    "tolerance and optimized execution. \n",
    "\n",
    "‚Ä¢ Significance: \n",
    "o Tracks lineage for fault recovery. \n",
    "o Optimizes execution by combining transformations. \n",
    " \n",
    "4Ô∏è‚É£3Ô∏è‚É£ Significance of Catalyst Optimizer in PySpark? \n",
    "\n",
    "‚Ä¢ What It Is: A query optimization engine in Spark SQL. \n",
    "‚Ä¢ Functions: \n",
    "o Converts logical plans into optimized physical plans. \n",
    "o Pushes predicates (filter operations) early to minimize I/O. \n",
    "‚Ä¢ Benefits: Better performance with optimized execution plans. \n",
    " \n",
    "\n",
    "4Ô∏è‚É£4Ô∏è‚É£ Query to Find the 4th Highest Salary of an Employee:\n",
    "\n",
    "SELECT DISTINCT salary \n",
    "FROM employee \n",
    "ORDER BY salary DESC \n",
    "LIMIT 4 OFFSET 3; \n",
    "\n",
    "\n",
    "‚Ä¢ Alternatively, using ROW_NUMBER: \n",
    "-----------------------------------\n",
    "\n",
    "SELECT salary  \n",
    "FROM ( \n",
    "  SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) AS rank \n",
    "  FROM employee  \n",
    ") ranked \n",
    "WHERE rank = 4; \n",
    " \n",
    "\n",
    "\n",
    "4Ô∏è‚É£5Ô∏è‚É£ PySpark Command to Read Data from a File into a DataFrame: \n",
    "\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True) \n",
    "\n",
    "‚Ä¢ Other Formats: \n",
    "o JSON: spark.read.json(\"path\")  \n",
    "o Parquet: spark.read.parquet(\"path\")  \n",
    " \n",
    "4Ô∏è‚É£6Ô∏è‚É£ Handling Nulls and Duplicates in PySpark: \n",
    "\n",
    "‚Ä¢ Drop Nulls: \n",
    "df = df.dropna() \n",
    "\n",
    "‚Ä¢ Fill Nulls: \n",
    "df = df.fillna({'col1': 'default_value', 'col2': 0}) \n",
    "\n",
    "\n",
    "‚Ä¢ Remove Duplicates: \n",
    "Page 13:\n",
    "    \n",
    "df = df.dropDuplicates(['col1', 'col2']) \n",
    " \n",
    "4Ô∏è‚É£7Ô∏è‚É£ Changing the Date Format for a Date Column: \n",
    "from pyspark.sql.functions import date_format \n",
    "df = df.withColumn(\"new_date\", date_format(\"date_column\", \"yyyy-MM-dd\")) \n",
    " \n",
    "4Ô∏è‚É£8Ô∏è‚É£ What is the Explode Function in PySpark? \n",
    "\n",
    "‚Ä¢ Explode: Converts an array or map into multiple rows. \n",
    "\n",
    "Example: \n",
    "---------\n",
    "\n",
    "‚Ä¢ from pyspark.sql.functions import explode \n",
    "df = df.withColumn(\"exploded_col\", explode(\"array_col\")) \n",
    " \n",
    "4Ô∏è‚É£9Ô∏è‚É£ Code to Read a Parquet File: \n",
    "df = spark.read.parquet(\"path/to/file.parquet\") \n",
    " \n",
    "5Ô∏è‚É£0Ô∏è‚É£ Code to Add a Column to a Parquet File: \n",
    "from pyspark.sql.functions import lit \n",
    "df = spark.read.parquet(\"path/to/file.parquet\") \n",
    "df = df.withColumn(\"new_column\", lit(\"value\")) \n",
    "df.write.parquet(\"path/to/updated_file.parquet\") \n",
    " \n",
    "5Ô∏è‚É£1Ô∏è‚É£ Different Approaches to Creating RDD in PySpark: \n",
    "\n",
    "‚Ä¢ From a Collection: \n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4]) \n",
    "\n",
    "‚Ä¢ From a File: \n",
    "rdd = spark.sparkContext.textFile(\"path/to/file.txt\") \n",
    " \n",
    "5Ô∏è‚É£2Ô∏è‚É£ Different Approaches to Creating DataFrame in PySpark: \n",
    "\n",
    "(i) ‚Ä¢ From RDD: \n",
    "from pyspark.sql import Row \n",
    "‚Ä¢ rdd = spark.sparkContext.parallelize([Row(name=\"Alice\", age=25), \n",
    "Row(name=\"Bob\", age=30)]) \n",
    "df = rdd.toDF() \n",
    "\n",
    "(ii) ‚Ä¢ From a File: \n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True) \n",
    "\n",
    "(iii) ‚Ä¢ From a List/Dictionary: \n",
    "\n",
    "‚Ä¢ data = [(\"Alice\", 25), (\"Bob\", 30)] \n",
    "df = spark.createDataFrame(data, schema=[\"name\", \"age\"]) \n",
    " \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Azure Data Engineer interview questions to  prepare!",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
